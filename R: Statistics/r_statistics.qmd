---
title: "R: Statistics"
format: pdf
editor: visual
---

Instructor: Rebecca Bergee, PhD

Date: June 20, 2024

$$\\[.01in]$$

## Research Computing Support (RCS)

We offer up to 15 hours of research assistance each semester to students and 50 hours per year for faculty and staff at UTK free of charge. This includes assistance with:

-   SAS, SPSS, JMP, R, for more please visit <https://oit.utk.edu/research/research-software/>

-   Research planning and design

-   Data entry and management, including file conversion

-   Statistical analysis and interpretation

-   Web survey design and deployment

-   Geographic information systems

See web page for details: <https://oit.utk.edu/research/>

$$\\[.1in]$$

## Preparing Your Computer

If you plan to following along and use the workshop files on your own computer, follow these steps:

1.  Install R from <http://www.r-project.org> and install RStudio from <http://www.rstudio.com>

2.  Download, unzip and save the workshop files in a single location on your machine

3.  Install these add-on packages by selecting the following "install.packages()" lines then choose "Run selected lines" from the "Run" menu

```{r, eval=F, echo=T}
install.packages("readxl")
install.packages("stats")
install.packages("dplyr")
install.packages("jmv")
install.packages("vcd")
install.packages("car")
install.packages("ggplot2")
install.packages("emmeans")
install.packages("multcomp")
install.packages("lme4")
install.packages("lmerTest")
```

$$\\[.1in]$$

## Part I: Workspace Setup, Importing & Printing Data

### Workspace Setup

The working directory is the first place your program looks for files. Therefore, you need to set the working directory of the file(s) using the location of the file on your local machine. You can either set the directory using code (example below) or manually set the directory by following the menu: "Session" \> "Set Working Directory" \> "Choose Directory..."

```{r}
setwd('C:/Users/rbergee/OneDrive - University of Tennessee/Workshops/r statistics') 
```

You can check that the working directory was set correctly using:

```{r}
getwd() 
```

***Note**: File paths may differ by software and/or operating system. Thus, pay attention to backslashes versus forward slashes when inserting a path.*

### Packages & Libraries

A package is a collection of R functions and code. Libraries are the storage containers for a package. To use a library, you need to first download and install the package, then load the library.

To install the package:

```{r, eval=F, echo=T}
install.packages()
```

***Note:** All packages should have been installed prior to this workshop. We will not be installing packages live.*

To load the libraries:

```{r, warning=FALSE, message=FALSE}
library("readxl")
library("stats")
library("dplyr")
library("jmv")
library("vcd")
library("car")
library("ggplot2")
library("emmeans")
library("multcomp")
library("lme4")
library("lmerTest")
```

### Importing & Printing Data

**Data Structure Tips:**

1.  Use a single row for each case or observation

2.  Use a column for each variable or measure

3.  R data sets are always rectangular

4.  Always use an ID number to trace bad values back to written records

5.  If measures are repeated across time, ID number *must* match subjects

**File Formats:**

| Software               | Extension |
|:-----------------------|:----------|
| Excel 2010             | XLSX      |
| Excel 2003-2007        | XLS       |
| SPSS                   | SAV       |
| Delimited File         | DLM       |
| Comma Separated Values | CSV       |
| JMP                    | JMP       |

Import an excel file using the `readxl` library:

```{r}
mtcars <- read_excel('mtcars.xlsx')
```

Import a csv file:

```{r, eval=F, echo=T}
mtcars <- read.csv('mtcars.csv')
```

Load R built-in data set:

```{r, eval=F, echo=T}
data(mtcars)
```

You can view your data by double-clicking the name of the data set in the "Environment" in the upper right section of RStudio. You can print the data using `print()` or print the first few rows using `head()` as so:

```{r}
head(mtcars, n=10)
```

***Note**: Do not print or open large data sets (\>1000 rows). Consider specifying the number of rows instead.*

$$\\[.1in]$$

## Part II: Descriptive Statistics

### Data Types

There are 5 data types in R:

1.  Numeric: numbers containing decimals

2.  Integer: numbers never containing decimals

3.  Character: text, known as strings

4.  Factor: categorical variable

5.  Logical: `TRUE` or `FALSE`

You can explore a data set by the structure, class, or specific data type. For example, we can look at all of the data types in `mtcars` using the structure function:

```{r}
  str(mtcars)
```

The class function will give the data type of a specific variable:

```{r}
  class(mtcars$wt)
```

You can also ask if a variable is a specific type:

```{r}
  is.numeric(mtcars$wt)
```

### Summary Statistics

The summary function in R lists the minimum, Q1, median, mean, Q3, and maximum values. If you insert the entire data set, it will list summary statistics for all variables in the set. To simplify, we can look at the summary statistics for a single variable:

```{r}
  summary(mtcars$mpg)
```

You can also extract values from the summary function or calculate the statistics using individual functions ( `min`, `quantile`, `median`, `mean`, `max`):

```{r}
summary(mtcars$mpg)[1]
```

```{r}
min(mtcars$mpg)
```

What about statistics not included in `summary` such as standard deviation, variance, or standard error?

```{r}
x <- mtcars$mpg

print(paste('standard deviation:', sd(x)))

print(paste('variance:', var(x)))
    
print(paste('standard error:', sd(x)/sqrt(length(x))))
```

### Missing Data

`summary` reports missing data if it is present in the data set.

```{r}
x <- c(2, NA, 3, 4, 5, 6, 7, 8)
summary(x)
```

You can also find missing data using:

```{r}
is.na(x)
```

or count the number of missing entries:

```{r}
sum(is.na(x))
```

### Descriptive & Frequency Tables

The `jmv` package offers a tabular form of the summary statistics using `descriptives`:

```{r}
descriptives(mtcars, vars=vars(mpg, cyl, disp, wt))
```

What about categorical variables? Before exploring frequency tables, we need to define factors in the data set:

```{r}
mtcars$vs <- as.factor(mtcars$vs)
```

We can explore factors using the `freq` option:

```{r}
descriptives(mtcars, vs, freq=TRUE)
```

***Note**: There are many ways to create tables in R. Those presented here are provided for ease of implementation and aesthetic.*

$$\\[.1in]$$

## Part III: Contingency Tables, Chi-Square Test & Correlations

### Contingency Tables (also known as cross tabulation)

Contingency tables display the frequency distribution of categorical variables. To illustrate, we will consider the `Arthritis` data from the `vcd` (Visualizing Categorical Data) package. First, let's load the data.

```{r}
data(Arthritis)
```

The data contains 84 observations representing independent patients in a clinical trial. The patient ID is recorded per observation along with the patient sex, age, treatment (placebo or treated) and level of improvement (None, Some, Marked). Let's build a contingency table using the `table` function:

```{r}
table(Arthritis$Treatment, Arthritis$Improved)
```

### Chi-Square Test

The Chi-Square test is used to test for an association between two variables in a two-way contingency table. Before the test can be conducted, we must first verify the assumptions:

1.  Both variables must be categorical and mutually exclusive, each subject must fall within exactly one cell.
2.  The observations must be independent and come from a random sample.
3.  The value of the expected cell counts should be greater than or equal to 5 in at least 80% of the cells, and no cell should have an expected count of less than one.

Now that the assumptions are verified, we can perform a Chi-Square test on our data. Given the contingency table built in the last section, let's consider the following hypotheses:

$H_0:$ Patient treatment and improvement are independent (no association)

$H_a:$ Patient treatment and improvement are associated

We are testing if an association exists between the two factors. To perform the Chi-Square test, let's first use the `chisq.test` function:

```{r}
chisq.test(Arthritis$Treatment, Arthritis$Improved)
```

Given the significant p-value, we reject the null and conclude that patient treatment is associated with patient improvement. An alternative option is to use `contTables` in the `jmv` package:

```{r}
contTables(Arthritis, Treatment, Improved)
```

**Other Categorical Functions**

-   Small Samples: `fisher.test` or set `fisher=TRUE` in `contTables`

-   Paired Samples: `mcnemar.test` or `contTablesPaired`

-   3-Way Table: `mantelhaen.test`

### Correlations

A correlation refers to the strength of the linear relationship between two continuous variables. The correlation coefficient, often denoted $\rho$, measures the magnitude and direction of the relationship. Before we discuss calculating correlations in R, let's first verify the assumptions of the data.

Assumptions:

1.  Linear Relationship
2.  Normally Distributed Variables

Consider the `mtcars` data. We can start by exploring the data visually to see if we suspect a linear relationship between any variables. Let's look at the data using the `pairs` function for a visual lead:

```{r}
pairs(mtcars[c('mpg', 'disp', 'hp', 'drat', 'wt', 'qsec')])
```

It appears visually that miles per gallon (mpg) and the weight (wt) of the vehicles have a linear relationship. Let's now test if those two variables are normally distributed.

```{r}
hist(mtcars$mpg)
shapiro.test(mtcars$mpg)

hist(mtcars$wt)
shapiro.test(mtcars$wt)
```

Notice that both pass the Shapiro-Wilk Test for normality and visually appear to have a normal distribution. Now that our assumptions are verified, we can move forward with testing the correlation between miles per gallon and weight of a vehicle. We can produce the correlation coefficient simply with the `cor` function:

```{r}
cor(mtcars$mpg, mtcars$wt, method='pearson')
```

This tells us that there is a strong negative relationship between the two variables. However, you can go a step further and use `cor.test` to statistically test the relationship and ensure that an association does in fact exist. Notice in the code below that we reject the null and conclude that the correlation is not equal to zero implying that an association exists.

```{r}
cor.test(mtcars$mpg, mtcars$wt, method='pearson')
```

What happens when our assumptions are violated? Consider non-parametric options such as the Spearman's rank (monotonic) or Kendall rank (small sample or outliers) correlation coefficients. In the `cor` function, specify using `method = c("pearson", "kendall", "spearman")`.

$$\\[.1in]$$

## Part IV: Linear Regression

Linear regression analysis is a statistical process in which one finds a line that best describes the relationship between variables. There are two main types of linear regression including:

1.  Simple Linear Regression (SLR): one independent variable (predictor) and one dependent variable (response)
2.  Multiple Linear Regression (MLR): two or more independent variables (predictors) and one dependent variable (response)

### Load & Prepare Data

We will continue with the `mtcars` data set. Let's start by reloading the data to discuss data preparation.

```{r}
mtcars <- read.csv('mtcars.csv')
```

Whether R provides the correct variable types or not for a data set, it is always a good idea to prepare the data by defining the continuous and categorical variables and saving the changes into a newly defined data set as so:

```{r}
# define continuous variables
mtcars$mpg <- as.numeric(mtcars$mpg)
mtcars$cyl <- as.numeric(mtcars$cyl)
mtcars$disp <- as.numeric(mtcars$disp)
mtcars$hp <- as.numeric(mtcars$hp)
mtcars$drat <- as.numeric(mtcars$drat)
mtcars$wt <- as.numeric(mtcars$wt)
mtcars$qsec <- as.numeric(mtcars$qsec)
mtcars$gear <- as.numeric(mtcars$gear)
mtcars$carb <- as.numeric(mtcars$carb)

# define categorical variables
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- as.factor(mtcars$am)

# save changes into new data set
mtcars_data <- mtcars
```

### Model Building

Let's say we are interested in determining which factors affect fuel efficiency. Build the linear model using the `lm` function:

```{r}
model_1 <- lm(mpg ~ wt + hp + disp + vs, data=mtcars_data)
```

***Note**: During the workshop, we may consider alternative models for demonstration purposes.*

### Verify Assumptions

There are five assumptions that need to be verified when implementing linear regression:

1.  Linearity
2.  Independent Observations
3.  No Multicollinearity
4.  Homoscedasticity of Residuals
5.  Normally Distributed Residuals

**Assumption 1: Linearity**

The number one assumption in **Linear** Regression is that you are modeling a linear relationship. You can verify this assumption visually by plotting the regression line.

```{r}
#store predicted values 
mtcars_data$preds <- predict(model_1)

#Regression Plot
ggplot(mtcars_data, aes(x=preds, y=mpg)) +
  geom_point(shape=1, size=3) +    # Use hollow circles
  geom_smooth(method=lm,   # Add linear regression line
              se=FALSE)    # Add shaded confidence region
```

**Assumption 2: Independent Observations**

As a researcher, you should not rely on a statistical test to determine independence of observations. Dependence can be determined using the residuals of the model. However, you are responsible for understanding the data collection process and should have collected the data from a random sample as independent observations. Occasionally, time series or longitudinal studies are mistaken as appropriate for regression. Alternative methods are required when considering dependent observations such as time series analysis or mixed-effects modeling.

**Assumption 3: No Multicollinearity (MLR Only)**

In Multiple Linear Regression, multiple independent variables (or predictors) enter the model. If predictors are correlated, the results are unreliable. You can test the correlations between predictors as we did in the correlations section of the workshop. However, the diagnostic used in regression during the modeling process is to check the variance inflation factor (VIF).

```{r}
vif(model_1)
```

VIF is an index for measuring the increase in variance of the regression coefficients due to collinearity in the model. If VIF is greater than 10, multicollinearity is high in the model. Another cutoff commonly used is VIF greater than 5. In our model, weight and displacement both have a VIF greater than 5 indicating that we need to consider removing one of the two variables.

As a researcher, you should consider the research question and remove the variable that practically makes sense in your field. As a statistician, we remove the variable with the lowest correlation to the response (dependent) variable.

```{r}
cor(mtcars$mpg, mtcars$wt, method='pearson')
cor(mtcars$mpg, mtcars$disp, method='pearson')
```

The correlations are both strong, but weight is slightly stronger. Therefore, I am choosing to drop displacement and keep weight in the model. If we were more interested in displacement, then we would remove weight instead. Let's build a new model to eliminate multicollinearity:

```{r}
model_2 <- lm(mpg ~ wt + hp + vs, data=mtcars_data)
```

**Assumption 4: Homoscedasticity of Residuals**

Once we get to this stage, we need to save the residuals, predicted values and standardized residuals of the current model to check the assumption:

```{r}
#store residuals
mtcars_data$resid <- resid(model_2)

#store predicted values 
mtcars_data$preds <- predict(model_2)

#store standardized residuals 
mtcars_data$stdresid <- rstandard(model_2)
```

Using these values stored in the data set, we will plot the residuals and standardized residuals by the predicted values to visually check that the residuals have constant variance. Furthermore, you can check DFBETAs and Cook's Distance to identify any influential observations.

```{r, warning=FALSE}

#Resdiual Vs Predicted values with center line at zero
ggplot(mtcars_data, aes(x=preds, y=resid)) +
  geom_point(shape=1, size=3) +    # Use hollow circles
  geom_hline(aes(yintercept=0))

#StdResdiual Vs Predicted values with center line at zero
ggplot(mtcars_data, aes(x=preds, y=stdresid)) +
  geom_point(shape=1, size=3) +    # Use hollow circles
  geom_hline(aes(yintercept=0))

infIndexPlot(model_2, grid=FALSE)
```

```{r, warning=FALSE}
dfbetas <- dfbetas(model_2, grid=FALSE) 

n <- nrow(mtcars)
thresh <- 2/sqrt(n)

which.max(dfbetas > thresh)[1]

```

If we want to remove specific values from the data, we can use the `subset` function and create a new data set moving forward:

```{r}
mtcars_reduced <- subset(mtcars_data, X != 20)
model_2 <- lm(mpg ~ wt + hp + vs, data=mtcars_reduced)
```

***Note:** Dropping data is not advised unless practical reasoning justifies the removal. We remove data in this example for demonstration purposes.*

**Assumption 5: Normally Distributed Residuals**

The last assumption can be verified using the Q-Q plot and the Shapiro-Wilk test for normality.

```{r}
#store residuals
mtcars_reduced$resid <- resid(model_2)

#Shapiro-Wilk test
shapiro.test(mtcars_reduced$resid)

#Q-Q plot with line
qqnorm(mtcars_reduced$resid)
qqline(mtcars_reduced$resid)
```

### Model Results

Now that the assumptions are verified, we can review the model results:

```{r}
summary(model_2)
```

You can also pull specific information from the model summary:

```{r}
AIC(model_2)
BIC(model_2)
summary(model_2)$coefficients
```

### Alternative Modeling Options

| Model                 | Formula                       |
|-----------------------|-------------------------------|
| Simple                | y \~ x                        |
| Without Intercept     | y \~ -1 + x                   |
| Include All Variables | y \~ .                        |
| Specify Interactions  | y \~ x1 + x2 + x1:x2          |
| or                    | y \~ x1\*x2                   |
| Polynomial            | y \~ x1 + I(x1\^2) + I(x1\^3) |
| or                    | y \~ poly(x1, 3)              |

------------------------------------------------------------------------

$$\\[.1in]$$

## Part V: t-Test, Analysis of Variance (ANOVA) & Mixed Model ANOVA

### Comparing Two Groups

There are multiple tests available to compare a measure (continuous variable) between two groups (factor with two levels). Your choice in test depends on the independence of the groups and the assumptions of the data.

**Independent Groups**

1.  Pooled t-Test: normally distributed response (Y), equal group variances
2.  Welch's t-Test: normally distributed response (Y), unequal group variances
3.  Mann-Whitney U test: response (Y) is not normally distributed

**Dependent Groups**

1.  Paired t-Test: group differences ($Y_1 - Y_2$) are normally distributed
2.  Wilcoxon signed-rank test: group differences ($Y_1 - Y_2$) are not normally distributed

**Implementation in R**

Let's consider the R built-in data set `PlantGrowth`. Say we are interested in comparing the weight of plants between treatment one and the control group. We should first load the data and subset the data to only include `trt1` and `ctrl` as so:

```{r}
data(PlantGrowth)
plant <- subset(PlantGrowth, group != 'trt2')
```

Now, we should assess the normality of the response variable, weight, and the group variances. You can use the Shapiro-Wilk test and Levene's test to check the assumptions as well as check visually using a histogram and box plot.

```{r}
hist(plant$weight)
shapiro.test(plant$weight)
leveneTest(weight ~ group, data=plant)
boxplot(plant$weight, plant$group)
```

This data represents independent groups with normally distributed weight and equal variance for the groups. Therefore, we can conduct the pooled t-Test for this data:

```{r}
t.test(weight~group, data=plant, var.equal=TRUE)
```

The `t.test` function automatically assumes unequal group variance and produces a Welch's t-test. You must set `var.equal=TRUE` to produce the two sample t-test. Another option mentioned above is the Mann-Whitney U test used when the response is not normally distributed. Let's assume the weight of the plants is not normally distributed and perform the Mann-Whitney U test:

```{r}
wilcox.test(weight~group, data=plant)
```

Note, the Mann-Whitney U test ranks the response variable for the calculation. Therefore, the error `warning: cannot compute exact p-value with ties` indicates that the p-value is approximated due to repeated values in the response ranked with ties.

Now that we have exhausted options for the independent samples. Let's move on to dependent or paired samples. Consider this mock data representing scores of students before (score1) and after (score2) the implementation of a new teaching method:

```{r}
data <- data.frame(ID = c(rep(1:20)), 
                   score1 = c(84 ,82, 78, 76, 92, 94, 90, 85, 71, 95,
                             84, 97, 98, 80, 90, 88, 95, 90, 96, 89),
                   score2 = c(84, 88, 88, 90, 92, 93, 91, 85, 80, 93,
                             97, 100, 93, 91, 92, 89, 94, 83, 92, 95))
```

As always, we first need to check our assumptions to determine if we need to implement a parametric or nonparametric test. Let's verify normality for the differences in the response:

```{r}
differences <- data$score1 - data$score2
hist(differences)
shapiro.test(differences)
```

The difference in score between the pre and post tests is normally distributed. Therefore, we will conduct a paired t-test:

```{r}
t.test(data$score1, data$score2, paired = TRUE)
```

If the differences were not normally distributed, we would have used the Wilcoxon signed-rank test as so:

```{r}
wilcox.test(data$score1, data$score2, paired = TRUE)
```

Note, the Wilcoxon test ranks the response variable for the calculation. Therefore, the error `warning: cannot compute exact p-value with ties` indicates that the p-value is approximated due to repeated values in the response ranked with ties. Furthermore, the error `warning: cannot compute exact p-value with zeroes` indicates that there are differences of zero in the data which also causes the calculation of the p-value to be approximated.

### Comparing More Than Two Groups

The ANOVA model is used when comparing a measure (continuous variable) between more than two groups (factor with more than two levels). In this section, we will again consider the `PlantGrowth` data which measures the `weight` of the plants classified by the factor `group` containing three levels: `trt1`, `trt2`, `ctrl`. Let's begin by preparing the workspace and data.

```{r}
# set contrasts for ANOVA
options(contrasts=c("contr.sum", "contr.poly"))

# reload data
data(PlantGrowth)
```

`contr.sum` is an option in R that sets contrasts as "sum to zero". This is needed in ANOVA modeling for contrasts to compare differences between each group to a baseline category known as pairwise comparisons. `contr.treatment` is the alternative option and what we use when specifying a regression model. It also compares each group to a baseline category but omits the category from the model (think of the process of creating dummy variables in regression).

`contr.poly` is used for both regression and ANOVA models and is used to return orthogonal contrasts.

```{r}
# create data set
plant <- PlantGrowth

# classify variables
plant$weight <- as.numeric(plant$weight)
plant$group <- as.factor(plant$group)

# relevel groups
plant$group <- relevel(plant$group, ref='ctrl')
```

Now that our workspace is set and we prepared the data, we can build the model and check our assumptions:

1.  Normally Distributed Response (Y)
2.  Equal Group Variance

```{r}
model <- lm(weight~group, data=plant)

#model assumptions
shapiro.test(summary(model)$residuals)
leveneTest(summary(model)$residuals, group=as.factor(plant$group))
```

The assumptions are met, so we can move forward with the model. We need to check if the factor is significant in the model using the `Anova` function from the `car` package:

```{r}
Anova(model, type=3)
```

Group is significant in the ANOVA table indicating that differences do exist between the groups for the plant weight. However, we cannot at this point state which groups differ which requires post hoc analysis. We can conduct post hoc tests using the `emmeans` and `multcomp` packages:

```{r}
#post-hoc comparisons
emm1<- emmeans(model, pairwise~group)
contrast(emm1, method='pairwise', adjust='tukey')
cld(emm1, Letters=LETTERS)
```

You can also compare all groups to a control group to minimize the number of pairwise comparisons performed using Dunnett's test:

```{r}
contrast(emm1, method='dunnett')
```

***Note:** Dunnett's test uses the reference category of the factor specified by R. This is why we `relevel` and clean the data before analysis.*

### Comparing Multiple Factors

What happens when you want to consider more than one factor with multiple levels? We can add fixed effects to the model as so:

| Model               | Formula            | Example: `mtcars`      |
|---------------------|--------------------|------------------------|
| One-way ANOVA       | y \~ a             | mpg \~ vs              |
| Two-way ANOVA       | y \~ a + b + a:b   | mpg \~ vs + am + vs:am |
| or                  | y \~ a\*b          | mpg \~ vs\*am          |
| 3-way ANOVA         | y \~ a\*b\*c       | mpg \~ vs\*am\*gear    |
| Nesting: b within a | y \~ a/b           |                        |
| or                  | y \~ y \~ b %in% a |                        |

***Note:*** *Factors incorporated this far are fixed effects. Random effects will be discussed in the next section.*

### Mixed Model ANOVA

The final topic within ANOVA is mixed modeling which includes both fixed and random effects. These types of models are used when more advanced designs are implemented. A few examples include split-plot designs, repeated measures designs, designs with sampling and/or sub-sampling, and hierarchical modeling. In order to properly build a model that best fits your data, you need a complete understanding of the data collection process, experimental design and primary research question.

**Definitions:**

-   *Fixed Effect:* treatments specifically chosen by the scientist that are repeatable and controllable no matter how many times the experiment is run

-   *Random Effect:* levels in the data that cause noise or additional variability that needs to be accounted for in the model

Repeated measures data will be provided in the supplementary materials. We will cover the model depending on timing and the interest of the audience. For reference, the following table is a guide to coding random effects provided by Bolker (2024):

| Formula                                                                        | Meaning                                                                                                                                 |
|--------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| `(1|group)`                                                                    | random group intercept                                                                                                                  |
| `(x|group)` = `(1+x|group)`                                                    | random slope of x within group with correlated intercept                                                                                |
| `(0+x|group)` = `(-1+x|group)`                                                 | random slope of x within group: no variation in intercept                                                                               |
| `(1|group) + (0+x|group)`                                                      | uncorrelated random intercept and random slope within group                                                                             |
| `(1|site/block)` = `(1|site)+(1|site:block)`                                   | intercept varying among sites and among blocks within sites (nested random effects)                                                     |
| `site+(1|site:block)`                                                          | *fixed* effect of sites plus random variation in intercept among blocks within sites                                                    |
| `(x|site/block)` = `(x|site)+(x|site:block)` = `(1 + x|site)+(1+x|site:block)` | slope and intercept varying among sites and among blocks within sites                                                                   |
| `(x1|site)+(x2|block)`                                                         | two different effects, varying at different levels                                                                                      |
| `x*site+(x|site:block)`                                                        | fixed effect variation of slope and intercept varying among sites and random variation of slope and intercept among blocks within sites |
| `(1|group1)+(1|group2)`                                                        | intercept varying among crossed random effects (e.g. site, year)                                                                        |

Bolker, Ben. (2024, June 20). GLMM FAQ. Retreieved from <https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html>\
$$\\[.1in]$$

# Thank You

## Questions?

**Email:** $\href{mailto:rbergee@utk.edu}{rbergee@utk.edu}$

**Provide Feedback:** <http://workshop.utk.edu>
